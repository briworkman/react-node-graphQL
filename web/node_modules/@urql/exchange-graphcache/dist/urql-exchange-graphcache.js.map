{"version":3,"file":"urql-exchange-graphcache.js","sources":["../src/ast/traversal.ts","../src/ast/schemaPredicates.ts","../src/operations/shared.ts","../src/operations/write.ts","../src/ast/node.ts","../src/ast/variables.ts","../src/helpers/help.ts","../src/store/data.ts","../src/store/store.ts","../src/operations/invalidate.ts","../src/operations/query.ts","../src/cacheExchange.ts","../src/store/keys.ts","../src/helpers/dict.ts","../src/offlineExchange.ts"],"sourcesContent":["import {\n  SelectionNode,\n  DefinitionNode,\n  DocumentNode,\n  FragmentDefinitionNode,\n  OperationDefinitionNode,\n  valueFromASTUntyped,\n  Kind,\n} from 'graphql';\n\nimport { getName } from './node';\n\nimport { invariant } from '../helpers/help';\nimport { Fragments, Variables } from '../types';\n\nconst isFragmentNode = (node: DefinitionNode): node is FragmentDefinitionNode =>\n  node.kind === Kind.FRAGMENT_DEFINITION;\n\n/** Returns the main operation's definition */\nexport const getMainOperation = (\n  doc: DocumentNode\n): OperationDefinitionNode => {\n  const operation = doc.definitions.find(\n    node => node.kind === Kind.OPERATION_DEFINITION\n  ) as OperationDefinitionNode;\n\n  invariant(\n    !!operation,\n    'Invalid GraphQL document: All GraphQL documents must contain an OperationDefinition' +\n      'node for a query, subscription, or mutation.',\n    1\n  );\n\n  return operation;\n};\n\n/** Returns a mapping from fragment names to their selections */\nexport const getFragments = (doc: DocumentNode): Fragments =>\n  doc.definitions.filter(isFragmentNode).reduce((map: Fragments, node) => {\n    map[getName(node)] = node;\n    return map;\n  }, {});\n\nexport const shouldInclude = (\n  node: SelectionNode,\n  vars: Variables\n): boolean => {\n  const { directives } = node;\n  if (!directives) return true;\n\n  // Finds any @include or @skip directive that forces the node to be skipped\n  for (let i = 0, l = directives.length; i < l; i++) {\n    const directive = directives[i];\n    const name = getName(directive);\n\n    if (\n      (name === 'include' || name === 'skip') &&\n      directive.arguments &&\n      directive.arguments[0] &&\n      getName(directive.arguments[0]) === 'if'\n    ) {\n      // Return whether this directive forces us to skip\n      // `@include(if: false)` or `@skip(if: true)`\n      const value = valueFromASTUntyped(directive.arguments[0].value, vars);\n      return name === 'include' ? !!value : !value;\n    }\n  }\n\n  return true;\n};\n","import {\n  isNullableType,\n  isListType,\n  isNonNullType,\n  GraphQLSchema,\n  GraphQLAbstractType,\n  GraphQLObjectType,\n  GraphQLInterfaceType,\n  GraphQLUnionType,\n} from 'graphql';\n\nimport { warn, invariant } from '../helpers/help';\nimport {\n  KeyingConfig,\n  UpdatesConfig,\n  ResolverConfig,\n  OptimisticMutationConfig,\n} from '../types';\n\nconst BUILTIN_FIELD_RE = /^__/;\n\nexport const isFieldNullable = (\n  schema: GraphQLSchema,\n  typename: string,\n  fieldName: string\n): boolean => {\n  if (BUILTIN_FIELD_RE.test(fieldName)) return true;\n  const field = getField(schema, typename, fieldName);\n  return !!field && isNullableType(field.type);\n};\n\nexport const isListNullable = (\n  schema: GraphQLSchema,\n  typename: string,\n  fieldName: string\n): boolean => {\n  const field = getField(schema, typename, fieldName);\n  if (!field) return false;\n  const ofType = isNonNullType(field.type) ? field.type.ofType : field.type;\n  return isListType(ofType) && isNullableType(ofType.ofType);\n};\n\nexport const isFieldAvailableOnType = (\n  schema: GraphQLSchema,\n  typename: string,\n  fieldName: string\n): boolean => {\n  if (BUILTIN_FIELD_RE.test(fieldName)) return true;\n  return !!getField(schema, typename, fieldName);\n};\n\nexport const isInterfaceOfType = (\n  schema: GraphQLSchema,\n  typeCondition: null | string,\n  typename: string | void\n): boolean => {\n  if (!typename || !typeCondition) return false;\n  if (typename === typeCondition) return true;\n\n  const abstractType = schema.getType(typeCondition);\n  const objectType = schema.getType(typename);\n\n  if (abstractType instanceof GraphQLObjectType) {\n    return abstractType === objectType;\n  }\n\n  expectAbstractType(abstractType, typeCondition);\n  expectObjectType(objectType, typename);\n  return schema.isPossibleType(abstractType, objectType);\n};\n\nconst getField = (\n  schema: GraphQLSchema,\n  typename: string,\n  fieldName: string\n) => {\n  const object = schema.getType(typename);\n  expectObjectType(object, typename);\n\n  const field = object.getFields()[fieldName];\n  if (!field) {\n    warn(\n      'Invalid field: The field `' +\n        fieldName +\n        '` does not exist on `' +\n        typename +\n        '`, ' +\n        'but the GraphQL document expects it to exist.\\n' +\n        'Traversal will continue, however this may lead to undefined behavior!',\n      4\n    );\n  }\n\n  return field;\n};\n\nfunction expectObjectType(\n  x: any,\n  typename: string\n): asserts x is GraphQLObjectType {\n  invariant(\n    x instanceof GraphQLObjectType,\n    'Invalid Object type: The type `' +\n      typename +\n      '` is not an object in the defined schema, ' +\n      'but the GraphQL document is traversing it.',\n    3\n  );\n}\n\nfunction expectAbstractType(\n  x: any,\n  typename: string\n): asserts x is GraphQLAbstractType {\n  invariant(\n    x instanceof GraphQLInterfaceType || x instanceof GraphQLUnionType,\n    'Invalid Abstract type: The type `' +\n      typename +\n      '` is not an Interface or Union type in the defined schema, ' +\n      'but a fragment in the GraphQL document is using it as a type condition.',\n    5\n  );\n}\n\nexport function expectValidKeyingConfig(\n  schema: GraphQLSchema,\n  keys: KeyingConfig\n): void {\n  if (process.env.NODE_ENV !== 'production') {\n    const types = Object.keys(schema.getTypeMap());\n    Object.keys(keys).forEach(key => {\n      if (types.indexOf(key) === -1) {\n        warn(\n          'Invalid Object type: The type `' +\n            key +\n            '` is not an object in the defined schema, but the `keys` option is referencing it.',\n          20\n        );\n      }\n    });\n  }\n}\n\nexport function expectValidUpdatesConfig(\n  schema: GraphQLSchema,\n  updates: UpdatesConfig\n): void {\n  if (process.env.NODE_ENV === 'production') {\n    return;\n  }\n\n  /* eslint-disable prettier/prettier */\n  const schemaMutations = schema.getMutationType()\n    ? Object.keys((schema.getMutationType() as GraphQLObjectType).toConfig().fields)\n    : [];\n  const schemaSubscriptions = schema.getSubscriptionType()\n    ? Object.keys((schema.getSubscriptionType() as GraphQLObjectType).toConfig().fields)\n    : [];\n  const givenMutations = updates.Mutation\n    ? Object.keys(updates.Mutation)\n    : [];\n  const givenSubscriptions = updates.Subscription\n    ? Object.keys(updates.Subscription)\n    : [];\n  /* eslint-enable prettier/prettier */\n\n  for (const givenMutation of givenMutations) {\n    if (schemaMutations.indexOf(givenMutation) === -1) {\n      warn(\n        'Invalid mutation field: `' +\n          givenMutation +\n          '` is not in the defined schema, but the `updates.Mutation` option is referencing it.',\n        21\n      );\n    }\n  }\n\n  for (const givenSubscription of givenSubscriptions) {\n    if (schemaSubscriptions.indexOf(givenSubscription) === -1) {\n      warn(\n        'Invalid subscription field: `' +\n          givenSubscription +\n          '` is not in the defined schema, but the `updates.Subscription` option is referencing it.',\n        22\n      );\n    }\n  }\n}\n\nfunction warnAboutResolver(name: string): void {\n  warn(\n    `Invalid resolver: \\`${name}\\` is not in the defined schema, but the \\`resolvers\\` option is referencing it.`,\n    23\n  );\n}\n\nexport function expectValidResolversConfig(\n  schema: GraphQLSchema,\n  resolvers: ResolverConfig\n): void {\n  if (process.env.NODE_ENV === 'production') {\n    return;\n  }\n\n  const validTypes = Object.keys(schema.getTypeMap());\n\n  for (const key in resolvers) {\n    if (key === 'Query') {\n      const queryType = schema.getQueryType();\n      if (queryType) {\n        const validQueries = Object.keys(queryType.toConfig().fields);\n        for (const resolverQuery in resolvers.Query) {\n          if (validQueries.indexOf(resolverQuery) === -1) {\n            warnAboutResolver('Query.' + resolverQuery);\n          }\n        }\n      } else {\n        warnAboutResolver('Query');\n      }\n    } else {\n      if (validTypes.indexOf(key) === -1) {\n        warnAboutResolver(key);\n      } else {\n        const validTypeProperties = Object.keys(\n          (schema.getType(key) as GraphQLObjectType).getFields()\n        );\n        const resolverProperties = Object.keys(resolvers[key]);\n        for (const resolverProperty of resolverProperties) {\n          if (validTypeProperties.indexOf(resolverProperty) === -1) {\n            warnAboutResolver(key + '.' + resolverProperty);\n          }\n        }\n      }\n    }\n  }\n}\n\nexport function expectValidOptimisticMutationsConfig(\n  schema: GraphQLSchema,\n  optimisticMutations: OptimisticMutationConfig\n): void {\n  if (process.env.NODE_ENV === 'production') {\n    return;\n  }\n\n  const validMutations = schema.getMutationType()\n    ? Object.keys(\n        (schema.getMutationType() as GraphQLObjectType).toConfig().fields\n      )\n    : [];\n\n  for (const mutation in optimisticMutations) {\n    if (validMutations.indexOf(mutation) === -1) {\n      warn(\n        `Invalid optimistic mutation field: \\`${mutation}\\` is not a mutation field in the defined schema, but the \\`optimistic\\` option is referencing it.`,\n        24\n      );\n    }\n  }\n}\n","import { InlineFragmentNode, FragmentDefinitionNode } from 'graphql';\n\nimport {\n  isInlineFragment,\n  getTypeCondition,\n  getSelectionSet,\n  getName,\n  SelectionSet,\n  isFieldNode,\n} from '../ast';\n\nimport { warn, pushDebugNode, popDebugNode } from '../helpers/help';\nimport { hasField } from '../store/data';\nimport { Store, keyOfField } from '../store';\nimport { Fragments, Variables, DataField, NullArray, Data } from '../types';\nimport { getFieldArguments, shouldInclude, isInterfaceOfType } from '../ast';\n\nexport interface Context {\n  store: Store;\n  variables: Variables;\n  fragments: Fragments;\n  parentTypeName: string;\n  parentKey: string;\n  parentFieldKey: string;\n  fieldName: string;\n  partial: boolean;\n  optimistic: boolean;\n}\n\nexport const makeContext = (\n  store: Store,\n  variables: Variables,\n  fragments: Fragments,\n  typename: string,\n  entityKey: string,\n  optimistic?: boolean\n): Context => ({\n  store,\n  variables,\n  fragments,\n  parentTypeName: typename,\n  parentKey: entityKey,\n  parentFieldKey: '',\n  fieldName: '',\n  partial: false,\n  optimistic: !!optimistic,\n});\n\nexport const updateContext = (\n  ctx: Context,\n  typename: string,\n  entityKey: string,\n  fieldKey: string,\n  fieldName: string\n) => {\n  ctx.parentTypeName = typename;\n  ctx.parentKey = entityKey;\n  ctx.parentFieldKey = fieldKey;\n  ctx.fieldName = fieldName;\n};\n\nconst isFragmentHeuristicallyMatching = (\n  node: InlineFragmentNode | FragmentDefinitionNode,\n  typename: void | string,\n  entityKey: string,\n  vars: Variables\n) => {\n  if (!typename) return false;\n  const typeCondition = getTypeCondition(node);\n  if (typename === typeCondition) return true;\n\n  warn(\n    'Heuristic Fragment Matching: A fragment is trying to match against the `' +\n      typename +\n      '` type, ' +\n      'but the type condition is `' +\n      typeCondition +\n      '`. Since GraphQL allows for interfaces `' +\n      typeCondition +\n      '` may be an' +\n      'interface.\\nA schema needs to be defined for this match to be deterministic, ' +\n      'otherwise the fragment will be matched heuristically!',\n    16\n  );\n\n  return !getSelectionSet(node).some(node => {\n    if (!isFieldNode(node)) return false;\n    const fieldKey = keyOfField(getName(node), getFieldArguments(node, vars));\n    return !hasField(entityKey, fieldKey);\n  });\n};\n\nexport const makeSelectionIterator = (\n  typename: void | string,\n  entityKey: string,\n  select: SelectionSet,\n  ctx: Context\n) => {\n  const indexStack: number[] = [0];\n  const selectionStack: SelectionSet[] = [select];\n\n  return {\n    next() {\n      while (indexStack.length !== 0) {\n        const index = indexStack[indexStack.length - 1]++;\n        const select = selectionStack[selectionStack.length - 1];\n        if (index >= select.length) {\n          indexStack.pop();\n          selectionStack.pop();\n          if (process.env.NODE_ENV !== 'production') {\n            popDebugNode();\n          }\n          continue;\n        } else {\n          const node = select[index];\n          if (!shouldInclude(node, ctx.variables)) {\n            continue;\n          } else if (!isFieldNode(node)) {\n            // A fragment is either referred to by FragmentSpread or inline\n            const fragmentNode = !isInlineFragment(node)\n              ? ctx.fragments[getName(node)]\n              : node;\n\n            if (fragmentNode !== undefined) {\n              if (process.env.NODE_ENV !== 'production') {\n                pushDebugNode(typename, fragmentNode);\n              }\n\n              const isMatching = ctx.store.schema\n                ? isInterfaceOfType(\n                    ctx.store.schema,\n                    getTypeCondition(fragmentNode),\n                    typename\n                  )\n                : isFragmentHeuristicallyMatching(\n                    fragmentNode,\n                    typename,\n                    entityKey,\n                    ctx.variables\n                  );\n\n              if (isMatching) {\n                indexStack.push(0);\n                selectionStack.push(getSelectionSet(fragmentNode));\n              }\n            }\n\n            continue;\n          } else if (getName(node) === '__typename') {\n            continue;\n          } else {\n            return node;\n          }\n        }\n      }\n\n      return undefined;\n    },\n  };\n};\n\nexport const ensureData = (x: DataField): Data | NullArray<Data> | null =>\n  x === undefined ? null : (x as Data | NullArray<Data>);\n","import { FieldNode, DocumentNode, FragmentDefinitionNode } from 'graphql';\n\nimport {\n  getFragments,\n  getMainOperation,\n  normalizeVariables,\n  getFieldArguments,\n  isFieldAvailableOnType,\n  getSelectionSet,\n  getName,\n  SelectionSet,\n  getFragmentTypeName,\n  getFieldAlias,\n} from '../ast';\n\nimport { invariant, warn, pushDebugNode, popDebugNode } from '../helpers/help';\n\nimport {\n  NullArray,\n  Variables,\n  Data,\n  Link,\n  OperationRequest,\n  Dependencies,\n  EntityField,\n} from '../types';\n\nimport {\n  Store,\n  getCurrentDependencies,\n  initDataState,\n  clearDataState,\n  joinKeys,\n  keyOfField,\n} from '../store';\n\nimport * as InMemoryData from '../store/data';\nimport {\n  Context,\n  makeSelectionIterator,\n  ensureData,\n  makeContext,\n  updateContext,\n} from './shared';\n\nexport interface WriteResult {\n  data: null | Data;\n  dependencies: Dependencies;\n}\n\n/** Writes a request given its response to the store */\nexport const write = (\n  store: Store,\n  request: OperationRequest,\n  data: Data,\n  key?: number\n): WriteResult => {\n  initDataState('write', store.data, key || null);\n  const result = startWrite(store, request, data);\n  clearDataState();\n  return result;\n};\n\nexport const startWrite = (\n  store: Store,\n  request: OperationRequest,\n  data: Data\n) => {\n  const operation = getMainOperation(request.query);\n  const result: WriteResult = { data, dependencies: getCurrentDependencies() };\n  const operationName = store.rootFields[operation.operation];\n\n  const ctx = makeContext(\n    store,\n    normalizeVariables(operation, request.variables),\n    getFragments(request.query),\n    operationName,\n    operationName\n  );\n\n  if (process.env.NODE_ENV !== 'production') {\n    pushDebugNode(operationName, operation);\n  }\n\n  writeSelection(ctx, operationName, getSelectionSet(operation), data);\n\n  if (process.env.NODE_ENV !== 'production') {\n    popDebugNode();\n  }\n\n  return result;\n};\n\nexport const writeOptimistic = (\n  store: Store,\n  request: OperationRequest,\n  key: number\n): WriteResult => {\n  initDataState('write', store.data, key, true);\n\n  const operation = getMainOperation(request.query);\n  const result: WriteResult = {\n    data: {} as Data,\n    dependencies: getCurrentDependencies(),\n  };\n  const operationName = store.rootFields[operation.operation];\n\n  invariant(\n    operationName === store.rootFields['mutation'],\n    'writeOptimistic(...) was called with an operation that is not a mutation.\\n' +\n      'This case is unsupported and should never occur.',\n    10\n  );\n\n  if (process.env.NODE_ENV !== 'production') {\n    pushDebugNode(operationName, operation);\n  }\n\n  const ctx = makeContext(\n    store,\n    normalizeVariables(operation, request.variables),\n    getFragments(request.query),\n    operationName,\n    operationName,\n    true\n  );\n\n  writeSelection(ctx, operationName, getSelectionSet(operation), result.data!);\n\n  if (process.env.NODE_ENV !== 'production') {\n    popDebugNode();\n  }\n\n  clearDataState();\n  return result;\n};\n\nexport const writeFragment = (\n  store: Store,\n  query: DocumentNode,\n  data: Partial<Data>,\n  variables?: Variables\n) => {\n  const fragments = getFragments(query);\n  const names = Object.keys(fragments);\n  const fragment = fragments[names[0]] as FragmentDefinitionNode;\n  if (fragment === undefined) {\n    return warn(\n      'writeFragment(...) was called with an empty fragment.\\n' +\n        'You have to call it with at least one fragment in your GraphQL document.',\n      11\n    );\n  }\n\n  const typename = getFragmentTypeName(fragment);\n  const dataToWrite = { __typename: typename, ...data } as Data;\n  const entityKey = store.keyOfEntity(dataToWrite);\n  if (!entityKey) {\n    return warn(\n      \"Can't generate a key for writeFragment(...) data.\\n\" +\n        'You have to pass an `id` or `_id` field or create a custom `keys` config for `' +\n        typename +\n        '`.',\n      12\n    );\n  }\n\n  if (process.env.NODE_ENV !== 'production') {\n    pushDebugNode(typename, fragment);\n  }\n\n  const ctx = makeContext(\n    store,\n    variables || {},\n    fragments,\n    typename,\n    entityKey\n  );\n\n  writeSelection(ctx, entityKey, getSelectionSet(fragment), dataToWrite);\n\n  if (process.env.NODE_ENV !== 'production') {\n    popDebugNode();\n  }\n};\n\nconst writeSelection = (\n  ctx: Context,\n  entityKey: undefined | string,\n  select: SelectionSet,\n  data: Data\n) => {\n  const isQuery = entityKey === ctx.store.rootFields['query'];\n  const isRoot = !isQuery && !!ctx.store.rootNames[entityKey!];\n  const typename = isRoot || isQuery ? entityKey : data.__typename;\n  if (!typename) {\n    return;\n  } else if (!isRoot && !isQuery && entityKey) {\n    InMemoryData.writeRecord(entityKey, '__typename', typename);\n  }\n\n  const iter = makeSelectionIterator(\n    typename,\n    entityKey || typename,\n    select,\n    ctx\n  );\n\n  let node: FieldNode | void;\n  while ((node = iter.next())) {\n    const fieldName = getName(node);\n    const fieldArgs = getFieldArguments(node, ctx.variables);\n    const fieldKey = keyOfField(fieldName, fieldArgs);\n    const fieldAlias = getFieldAlias(node);\n    let fieldValue = data[fieldAlias];\n\n    if (process.env.NODE_ENV !== 'production') {\n      if (!isRoot && fieldValue === undefined) {\n        const advice = ctx.optimistic\n          ? '\\nYour optimistic result may be missing a field!'\n          : '';\n\n        const expected =\n          node.selectionSet === undefined\n            ? 'scalar (number, boolean, etc)'\n            : 'selection set';\n\n        warn(\n          'Invalid undefined: The field at `' +\n            fieldKey +\n            '` is `undefined`, but the GraphQL query expects a ' +\n            expected +\n            ' for this field.' +\n            advice,\n          13\n        );\n\n        continue; // Skip this field\n      } else if (ctx.store.schema && typename) {\n        isFieldAvailableOnType(ctx.store.schema, typename, fieldName);\n      }\n    }\n\n    if (ctx.optimistic && isRoot) {\n      const resolver = ctx.store.optimisticMutations[fieldName];\n\n      if (!resolver) continue;\n      // We have to update the context to reflect up-to-date ResolveInfo\n      updateContext(ctx, typename, typename, fieldKey, fieldName);\n      fieldValue = data[fieldAlias] = ensureData(\n        resolver(fieldArgs || {}, ctx.store, ctx)\n      );\n    }\n\n    if (node.selectionSet) {\n      // Process the field and write links for the child entities that have been written\n      if (entityKey && !isRoot) {\n        const key = joinKeys(entityKey, fieldKey);\n        const link = writeField(\n          ctx,\n          getSelectionSet(node),\n          ensureData(fieldValue),\n          key\n        );\n        InMemoryData.writeLink(entityKey || typename, fieldKey, link);\n      } else {\n        writeField(ctx, getSelectionSet(node), ensureData(fieldValue));\n      }\n    } else if (entityKey && !isRoot) {\n      // This is a leaf node, so we're setting the field's value directly\n      InMemoryData.writeRecord(\n        entityKey || typename,\n        fieldKey,\n        fieldValue as EntityField\n      );\n    }\n\n    if (isRoot) {\n      // We have to update the context to reflect up-to-date ResolveInfo\n      updateContext(\n        ctx,\n        typename,\n        typename,\n        joinKeys(typename, fieldKey),\n        fieldName\n      );\n\n      // We run side-effect updates after the default, normalized updates\n      // so that the data is already available in-store if necessary\n      const updater = ctx.store.updates[typename][fieldName];\n      if (updater) {\n        data[fieldName] = fieldValue;\n        updater(data, fieldArgs || {}, ctx.store, ctx);\n      }\n    }\n  }\n};\n\n// A pattern to match typenames of types that are likely never keyable\nconst KEYLESS_TYPE_RE = /^__|PageInfo|(Connection|Edge)$/;\n\nconst writeField = (\n  ctx: Context,\n  select: SelectionSet,\n  data: null | Data | NullArray<Data>,\n  parentFieldKey?: string\n): Link => {\n  if (Array.isArray(data)) {\n    const newData = new Array(data.length);\n    for (let i = 0, l = data.length; i < l; i++) {\n      const item = data[i];\n      // Append the current index to the parentFieldKey fallback\n      const indexKey = parentFieldKey\n        ? joinKeys(parentFieldKey, `${i}`)\n        : undefined;\n      // Recursively write array data\n      const links = writeField(ctx, select, item, indexKey);\n      // Link cannot be expressed as a recursive type\n      newData[i] = links as string | null;\n    }\n\n    return newData;\n  } else if (data === null) {\n    return null;\n  }\n\n  const entityKey = ctx.store.keyOfEntity(data);\n  const typename = data.__typename;\n\n  if (\n    parentFieldKey &&\n    ctx.store.keys[data.__typename] === undefined &&\n    entityKey === null &&\n    typeof typename === 'string' &&\n    !KEYLESS_TYPE_RE.test(typename)\n  ) {\n    warn(\n      'Invalid key: The GraphQL query at the field at `' +\n        parentFieldKey +\n        '` has a selection set, ' +\n        'but no key could be generated for the data at this field.\\n' +\n        'You have to request `id` or `_id` fields for all selection sets or create ' +\n        'a custom `keys` config for `' +\n        typename +\n        '`.\\n' +\n        'Entities without keys will be embedded directly on the parent entity. ' +\n        'If this is intentional, create a `keys` config for `' +\n        typename +\n        '` that always returns null.',\n      15\n    );\n  }\n\n  const childKey = entityKey || parentFieldKey;\n  writeSelection(ctx, childKey, select, data);\n  return childKey || null;\n};\n","import {\n  NamedTypeNode,\n  NameNode,\n  SelectionNode,\n  SelectionSetNode,\n  InlineFragmentNode,\n  FieldNode,\n  FragmentDefinitionNode,\n  Kind,\n} from 'graphql';\n\nexport type SelectionSet = ReadonlyArray<SelectionNode>;\n\n/** Returns the name of a given node */\nexport const getName = (node: { name: NameNode }): string => node.name.value;\n\nexport const getFragmentTypeName = (node: FragmentDefinitionNode): string =>\n  node.typeCondition.name.value;\n\n/** Returns either the field's name or the field's alias */\nexport const getFieldAlias = (node: FieldNode): string =>\n  node.alias ? node.alias.value : getName(node);\n\n/** Returns the SelectionSet for a given inline or defined fragment node */\nexport const getSelectionSet = (node: {\n  selectionSet?: SelectionSetNode;\n}): SelectionSet => (node.selectionSet ? node.selectionSet.selections : []);\n\nexport const getTypeCondition = ({\n  typeCondition,\n}: {\n  typeCondition?: NamedTypeNode;\n}): string | null => (typeCondition ? getName(typeCondition) : null);\n\nexport const isFieldNode = (node: SelectionNode): node is FieldNode =>\n  node.kind === Kind.FIELD;\n\nexport const isInlineFragment = (\n  node: SelectionNode\n): node is InlineFragmentNode => node.kind === Kind.INLINE_FRAGMENT;\n","import {\n  FieldNode,\n  OperationDefinitionNode,\n  valueFromASTUntyped,\n} from 'graphql';\n\nimport { getName } from './node';\n\nimport { Variables } from '../types';\n\n/** Evaluates a fields arguments taking vars into account */\nexport const getFieldArguments = (\n  node: FieldNode,\n  vars: Variables\n): null | Variables => {\n  const args = {};\n  let argsSize = 0;\n  if (node.arguments && node.arguments.length) {\n    for (let i = 0, l = node.arguments.length; i < l; i++) {\n      const arg = node.arguments[i];\n      const value = valueFromASTUntyped(arg.value, vars);\n      if (value !== undefined && value !== null) {\n        args[getName(arg)] = value;\n        argsSize++;\n      }\n    }\n  }\n\n  return argsSize > 0 ? args : null;\n};\n\n/** Returns a filtered form of variables with values missing that the query doesn't require */\nexport const filterVariables = (\n  node: OperationDefinitionNode,\n  input: void | object\n) => {\n  if (!input || !node.variableDefinitions) {\n    return undefined;\n  }\n\n  const vars = {};\n  for (let i = 0, l = node.variableDefinitions.length; i < l; i++) {\n    const name = getName(node.variableDefinitions[i].variable);\n    vars[name] = input[name];\n  }\n\n  return vars;\n};\n\n/** Returns a normalized form of variables with defaulted values */\nexport const normalizeVariables = (\n  node: OperationDefinitionNode,\n  input: void | object\n): Variables => {\n  const vars = {};\n  if (!input) return vars;\n\n  if (node.variableDefinitions) {\n    for (let i = 0, l = node.variableDefinitions.length; i < l; i++) {\n      const def = node.variableDefinitions[i];\n      const name = getName(def.variable);\n      vars[name] =\n        input[name] === undefined && def.defaultValue\n          ? valueFromASTUntyped(def.defaultValue, input)\n          : input[name];\n    }\n  }\n\n  for (const key in input) {\n    if (!(key in vars)) vars[key] = input[key];\n  }\n\n  return vars;\n};\n","// These are guards that are used throughout the codebase to warn or error on\n// unexpected behaviour or conditions.\n// Every warning and error comes with a number that uniquely identifies them.\n// You can read more about the messages themselves in `docs/graphcache/errors.md`\n\nimport { Kind, ExecutableDefinitionNode, InlineFragmentNode } from 'graphql';\n\nexport type ErrorCode =\n  | 1\n  | 2\n  | 3\n  | 4\n  | 5\n  | 6\n  | 7\n  | 8\n  | 9\n  | 10\n  | 11\n  | 12\n  | 13\n  | 15\n  | 16\n  | 17\n  | 18\n  | 19\n  | 20\n  | 21\n  | 22\n  | 23\n  | 24\n  | 25;\n\ntype DebugNode = ExecutableDefinitionNode | InlineFragmentNode;\n\n// URL unfurls to https://formidable.com/open-source/urql/docs/graphcache/errors/\nconst helpUrl = '\\nhttps://bit.ly/2XbVrpR#';\nconst cache = new Set<string>();\n\nexport const currentDebugStack: string[] = [];\n\nexport const popDebugNode = () => currentDebugStack.pop();\n\nexport const pushDebugNode = (typename: void | string, node: DebugNode) => {\n  let identifier = '';\n  if (node.kind === Kind.INLINE_FRAGMENT) {\n    identifier = typename\n      ? `Inline Fragment on \"${typename}\"`\n      : 'Inline Fragment';\n  } else if (node.kind === Kind.OPERATION_DEFINITION) {\n    const name = node.name ? `\"${node.name.value}\"` : 'Unnamed';\n    identifier = `${name} ${node.operation}`;\n  } else if (node.kind === Kind.FRAGMENT_DEFINITION) {\n    identifier = `\"${node.name.value}\" Fragment`;\n  }\n\n  if (identifier) {\n    currentDebugStack.push(identifier);\n  }\n};\n\nconst getDebugOutput = (): string =>\n  currentDebugStack.length\n    ? '\\n(Caused At: ' + currentDebugStack.join(', ') + ')'\n    : '';\n\nexport function invariant(\n  condition: any,\n  message: string,\n  code: ErrorCode\n): asserts condition {\n  if (!condition) {\n    let errorMessage = message || 'Minfied Error #' + code + '\\n';\n    if (process.env.NODE_ENV !== 'production') {\n      errorMessage += getDebugOutput();\n    }\n\n    const error = new Error(errorMessage + helpUrl + code);\n    error.name = 'Graphcache Error';\n    throw error;\n  }\n}\n\nexport function warn(message: string, code: ErrorCode) {\n  if (!cache.has(message)) {\n    console.warn(message + getDebugOutput() + helpUrl + code);\n    cache.add(message);\n  }\n}\n","import { stringifyVariables } from '@urql/core';\n\nimport {\n  Link,\n  EntityField,\n  FieldInfo,\n  StorageAdapter,\n  SerializedEntries,\n  Dependencies,\n  OperationType,\n} from '../types';\n\nimport {\n  serializeKeys,\n  deserializeKeyInfo,\n  fieldInfoOfKey,\n  joinKeys,\n} from './keys';\n\nimport { makeDict } from '../helpers/dict';\nimport { invariant, currentDebugStack } from '../helpers/help';\n\ntype Dict<T> = Record<string, T>;\ntype KeyMap<T> = Map<string, T>;\ntype OptimisticMap<T> = Record<number, T>;\n\ninterface NodeMap<T> {\n  optimistic: OptimisticMap<KeyMap<Dict<T | undefined>>>;\n  base: KeyMap<Dict<T>>;\n}\n\nexport interface InMemoryData {\n  /** Flag for whether deferred tasks have been scheduled yet */\n  defer: boolean;\n  /** A list of entities that have been flagged for gargabe collection since no references to them are left */\n  gc: Set<string>;\n  /** A list of entity+field keys that will be persisted */\n  persist: Set<string>;\n  /** The API's \"Query\" typename which is needed to filter dependencies */\n  queryRootKey: string;\n  /** Number of references to each entity (except \"Query\") */\n  refCount: Dict<number>;\n  /** Number of references to each entity on optimistic layers */\n  refLock: OptimisticMap<Dict<number>>;\n  /** A map of entity fields (key-value entries per entity) */\n  records: NodeMap<EntityField>;\n  /** A map of entity links which are connections from one entity to another (key-value entries per entity) */\n  links: NodeMap<Link>;\n  /** A set of Query operation keys that are in-flight and awaiting a result */\n  commutativeKeys: Set<number>;\n  /** The order of optimistic layers */\n  optimisticOrder: number[];\n  /** This may be a persistence adapter that will receive changes in a batch */\n  storage: StorageAdapter | null;\n}\n\nlet currentOperation: null | OperationType = null;\nlet currentData: null | InMemoryData = null;\nlet currentDependencies: null | Dependencies = null;\nlet currentOptimisticKey: null | number = null;\nlet currentIgnoreOptimistic = false;\n\nconst makeNodeMap = <T>(): NodeMap<T> => ({\n  optimistic: makeDict(),\n  base: new Map(),\n});\n\n/** Before reading or writing the global state needs to be initialised */\nexport const initDataState = (\n  operationType: OperationType,\n  data: InMemoryData,\n  layerKey: number | null,\n  isOptimistic?: boolean\n) => {\n  currentOperation = operationType;\n  currentData = data;\n  currentDependencies = makeDict();\n  currentIgnoreOptimistic = !!isOptimistic;\n  if (process.env.NODE_ENV !== 'production') {\n    currentDebugStack.length = 0;\n  }\n\n  if (!layerKey) {\n    currentOptimisticKey = null;\n  } else if (isOptimistic || data.optimisticOrder.length > 0) {\n    // If this operation isn't optimistic and we see it for the first time,\n    // then it must've been optimistic in the past, so we can proactively\n    // clear the optimistic data before writing\n    if (!isOptimistic && !data.commutativeKeys.has(layerKey)) {\n      reserveLayer(data, layerKey);\n    } else if (isOptimistic) {\n      // NOTE: This optimally shouldn't happen as it implies that an optimistic\n      // write is being performed after a concrete write.\n      data.commutativeKeys.delete(layerKey);\n    }\n\n    // An optimistic update of a mutation may force an optimistic layer,\n    // or this Query update may be applied optimistically since it's part\n    // of a commutate chain\n    currentOptimisticKey = layerKey;\n    createLayer(data, layerKey);\n  } else {\n    // Otherwise we don't create an optimistic layer and clear the\n    // operation's one if it already exists\n    currentOptimisticKey = null;\n    deleteLayer(data, layerKey);\n  }\n};\n\n/** Reset the data state after read/write is complete */\nexport const clearDataState = () => {\n  // NOTE: This is only called to check for the invariant to pass\n  if (process.env.NODE_ENV !== 'production') {\n    getCurrentDependencies();\n  }\n\n  const data = currentData!;\n  const layerKey = currentOptimisticKey;\n  currentIgnoreOptimistic = false;\n  currentOptimisticKey = null;\n\n  // Determine whether the current operation has been a commutative layer\n  if (layerKey && data.optimisticOrder.indexOf(layerKey) > -1) {\n    // Squash all layers in reverse order (low priority upwards) that have\n    // been written already\n    let i = data.optimisticOrder.length;\n    while (\n      --i >= 0 &&\n      data.refLock[data.optimisticOrder[i]] &&\n      data.commutativeKeys.has(data.optimisticOrder[i])\n    ) {\n      squashLayer(data.optimisticOrder[i]);\n    }\n  }\n\n  currentOperation = null;\n  currentData = null;\n  currentDependencies = null;\n  if (process.env.NODE_ENV !== 'production') {\n    currentDebugStack.length = 0;\n  }\n\n  // Schedule deferred tasks if we haven't already\n  if (process.env.NODE_ENV !== 'test' && !data.defer) {\n    data.defer = true;\n    Promise.resolve().then(() => {\n      initDataState('write', data, null);\n      gc();\n      persistData();\n      clearDataState();\n      data.defer = false;\n    });\n  }\n};\n\n/** Initialises then resets the data state, which may squash this layer if necessary */\nexport const noopDataState = (\n  data: InMemoryData,\n  layerKey: number | null,\n  isOptimistic?: boolean\n) => {\n  initDataState('read', data, layerKey, isOptimistic);\n  clearDataState();\n};\n\nexport const getCurrentOperation = (): OperationType => {\n  invariant(\n    currentOperation !== null,\n    'Invalid Cache call: The cache may only be accessed or mutated during' +\n      'operations like write or query, or as part of its resolvers, updaters, ' +\n      'or optimistic configs.',\n    2\n  );\n\n  return currentOperation;\n};\n\n/** As we're writing, we keep around all the records and links we've read or have written to */\nexport const getCurrentDependencies = (): Dependencies => {\n  invariant(\n    currentDependencies !== null,\n    'Invalid Cache call: The cache may only be accessed or mutated during' +\n      'operations like write or query, or as part of its resolvers, updaters, ' +\n      'or optimistic configs.',\n    2\n  );\n\n  return currentDependencies;\n};\n\nexport const make = (queryRootKey: string): InMemoryData => ({\n  defer: false,\n  gc: new Set(),\n  persist: new Set(),\n  queryRootKey,\n  refCount: makeDict(),\n  refLock: makeDict(),\n  links: makeNodeMap(),\n  records: makeNodeMap(),\n  commutativeKeys: new Set(),\n  optimisticOrder: [],\n  storage: null,\n});\n\n/** Adds a node value to a NodeMap (taking optimistic values into account */\nconst setNode = <T>(\n  map: NodeMap<T>,\n  entityKey: string,\n  fieldKey: string,\n  value: T\n) => {\n  // Optimistic values are written to a map in the optimistic dict\n  // All other values are written to the base map\n  const keymap: KeyMap<Dict<T | undefined>> = currentOptimisticKey\n    ? map.optimistic[currentOptimisticKey]\n    : map.base;\n\n  // On the map itself we get or create the entity as a dict\n  let entity = keymap.get(entityKey) as Dict<T | undefined>;\n  if (entity === undefined) {\n    keymap.set(entityKey, (entity = makeDict()));\n  }\n\n  // If we're setting undefined we delete the node's entry\n  // On optimistic layers we actually set undefined so it can\n  // override the base value\n  if (value === undefined && !currentOptimisticKey) {\n    delete entity[fieldKey];\n  } else {\n    entity[fieldKey] = value;\n  }\n};\n\n/** Gets a node value from a NodeMap (taking optimistic values into account */\nconst getNode = <T>(\n  map: NodeMap<T>,\n  entityKey: string,\n  fieldKey: string\n): T | undefined => {\n  let node: Dict<T | undefined> | undefined;\n\n  // This first iterates over optimistic layers (in order)\n  for (let i = 0, l = currentData!.optimisticOrder.length; i < l; i++) {\n    const layerKey = currentData!.optimisticOrder[i];\n    const optimistic = map.optimistic[layerKey];\n    // If the node and node value exists it is returned, including undefined\n    if (\n      optimistic &&\n      (!currentIgnoreOptimistic ||\n        currentData!.commutativeKeys.has(layerKey)) &&\n      (node = optimistic.get(entityKey)) !== undefined &&\n      fieldKey in node\n    ) {\n      return node[fieldKey];\n    }\n  }\n\n  // Otherwise we read the non-optimistic base value\n  node = map.base.get(entityKey);\n  return node !== undefined ? node[fieldKey] : undefined;\n};\n\n/** Adjusts the reference count of an entity on a refCount dict by \"by\" and updates the gc */\nconst updateRCForEntity = (\n  gc: void | Set<string>,\n  refCount: Dict<number>,\n  entityKey: string,\n  by: number\n): void => {\n  // Retrieve the reference count\n  const count = refCount[entityKey] !== undefined ? refCount[entityKey] : 0;\n  // Adjust it by the \"by\" value\n  const newCount = (refCount[entityKey] = (count + by) | 0);\n  // Add it to the garbage collection batch if it needs to be deleted or remove it\n  // from the batch if it needs to be kept\n  if (gc !== undefined) {\n    if (newCount <= 0) gc.add(entityKey);\n    else if (count <= 0 && newCount > 0) gc.delete(entityKey);\n  }\n};\n\n/** Adjusts the reference counts of all entities of a link on a refCount dict by \"by\" and updates the gc */\nconst updateRCForLink = (\n  gc: void | Set<string>,\n  refCount: Dict<number>,\n  link: Link | undefined,\n  by: number\n): void => {\n  if (typeof link === 'string') {\n    updateRCForEntity(gc, refCount, link, by);\n  } else if (Array.isArray(link)) {\n    for (let i = 0, l = link.length; i < l; i++) {\n      const entityKey = link[i];\n      if (entityKey) {\n        updateRCForEntity(gc, refCount, entityKey, by);\n      }\n    }\n  }\n};\n\n/** Writes all parsed FieldInfo objects of a given node dict to a given array if it hasn't been seen */\nconst extractNodeFields = <T>(\n  fieldInfos: FieldInfo[],\n  seenFieldKeys: Set<string>,\n  node: Dict<T> | undefined\n): void => {\n  if (node !== undefined) {\n    for (const fieldKey in node) {\n      if (!seenFieldKeys.has(fieldKey)) {\n        // If the node hasn't been seen the serialized fieldKey is turnt back into\n        // a rich FieldInfo object that also contains the field's name and arguments\n        fieldInfos.push(fieldInfoOfKey(fieldKey));\n        seenFieldKeys.add(fieldKey);\n      }\n    }\n  }\n};\n\n/** Writes all parsed FieldInfo objects of all nodes in a NodeMap to a given array */\nconst extractNodeMapFields = <T>(\n  fieldInfos: FieldInfo[],\n  seenFieldKeys: Set<string>,\n  entityKey: string,\n  map: NodeMap<T>\n) => {\n  // Extracts FieldInfo for the entity in the base map\n  extractNodeFields(fieldInfos, seenFieldKeys, map.base.get(entityKey));\n\n  // Then extracts FieldInfo for the entity from the optimistic maps\n  for (let i = 0, l = currentData!.optimisticOrder.length; i < l; i++) {\n    const optimistic = map.optimistic[currentData!.optimisticOrder[i]];\n    if (optimistic !== undefined) {\n      extractNodeFields(fieldInfos, seenFieldKeys, optimistic.get(entityKey));\n    }\n  }\n};\n\n/** Garbage collects all entities that have been marked as having no references */\nexport const gc = () => {\n  // Iterate over all entities that have been marked for deletion\n  // Entities have been marked for deletion in `updateRCForEntity` if\n  // their reference count dropped to 0\n  currentData!.gc.forEach((entityKey: string, _, batch: Set<string>) => {\n    // Check first whether the reference count is still 0\n    const rc = currentData!.refCount[entityKey] || 0;\n    if (rc > 0) {\n      batch.delete(entityKey);\n      return;\n    }\n\n    // Each optimistic layer may also still contain some references to marked entities\n    for (const layerKey in currentData!.refLock) {\n      const refCount = currentData!.refLock[layerKey];\n      const locks = refCount[entityKey] || 0;\n      // If the optimistic layer has any references to the entity, don't GC it,\n      // otherwise delete the reference count from the optimistic layer\n      if (locks > 0) return;\n      delete refCount[entityKey];\n    }\n\n    // Delete the reference count, and delete the entity from the GC batch\n    delete currentData!.refCount[entityKey];\n    batch.delete(entityKey);\n    currentData!.records.base.delete(entityKey);\n    const linkNode = currentData!.links.base.get(entityKey);\n    if (linkNode) {\n      currentData!.links.base.delete(entityKey);\n      for (const fieldKey in linkNode) {\n        updateRCForLink(batch, currentData!.refCount, linkNode[fieldKey], -1);\n      }\n    }\n  });\n};\n\nconst updateDependencies = (entityKey: string, fieldKey?: string) => {\n  if (fieldKey !== '__typename') {\n    if (entityKey !== currentData!.queryRootKey) {\n      currentDependencies![entityKey] = true;\n    } else if (fieldKey !== undefined) {\n      currentDependencies![joinKeys(entityKey, fieldKey)] = true;\n    }\n  }\n};\n\nconst updatePersist = (entityKey: string, fieldKey: string) => {\n  if (!currentIgnoreOptimistic && currentData!.storage) {\n    currentData!.persist.add(serializeKeys(entityKey, fieldKey));\n  }\n};\n\n/** Reads an entity's field (a \"record\") from data */\nexport const readRecord = (\n  entityKey: string,\n  fieldKey: string\n): EntityField => {\n  updateDependencies(entityKey, fieldKey);\n  return getNode(currentData!.records, entityKey, fieldKey);\n};\n\n/** Reads an entity's link from data */\nexport const readLink = (\n  entityKey: string,\n  fieldKey: string\n): Link | undefined => {\n  updateDependencies(entityKey, fieldKey);\n  return getNode(currentData!.links, entityKey, fieldKey);\n};\n\n/** Writes an entity's field (a \"record\") to data */\nexport const writeRecord = (\n  entityKey: string,\n  fieldKey: string,\n  value?: EntityField\n) => {\n  updateDependencies(entityKey, fieldKey);\n  updatePersist(entityKey, fieldKey);\n  setNode(currentData!.records, entityKey, fieldKey, value);\n};\n\nexport const hasField = (entityKey: string, fieldKey: string): boolean =>\n  readRecord(entityKey, fieldKey) !== undefined ||\n  readLink(entityKey, fieldKey) !== undefined;\n\n/** Writes an entity's link to data */\nexport const writeLink = (\n  entityKey: string,\n  fieldKey: string,\n  link?: Link | undefined\n) => {\n  const data = currentData!;\n  // Retrieve the reference counting dict or the optimistic reference locking dict\n  let refCount: Dict<number>;\n  // Retrive the link NodeMap from either an optimistic or the base layer\n  let links: KeyMap<Dict<Link | undefined>> | undefined;\n  // Set the GC batch if we're not optimistically updating\n  let gc: void | Set<string>;\n  if (currentOptimisticKey) {\n    // The refLock counters are also reference counters, but they prevent\n    // garbage collection instead of being used to trigger it\n    refCount =\n      data.refLock[currentOptimisticKey] ||\n      (data.refLock[currentOptimisticKey] = makeDict());\n    links = data.links.optimistic[currentOptimisticKey];\n  } else {\n    refCount = data.refCount;\n    links = data.links.base;\n    gc = data.gc;\n  }\n\n  // Retrieve the previous link for this field\n  const prevLinkNode = links && links.get(entityKey);\n  const prevLink = prevLinkNode && prevLinkNode[fieldKey];\n\n  // Update persistence batch and dependencies\n  updateDependencies(entityKey, fieldKey);\n  updatePersist(entityKey, fieldKey);\n  // Update the link\n  setNode(data.links, entityKey, fieldKey, link);\n  // First decrease the reference count for the previous link\n  updateRCForLink(gc, refCount, prevLink, -1);\n  // Then increase the reference count for the new link\n  updateRCForLink(gc, refCount, link, 1);\n};\n\n/** Reserves an optimistic layer and preorders it */\nexport const reserveLayer = (data: InMemoryData, layerKey: number) => {\n  const index = data.optimisticOrder.indexOf(layerKey);\n  if (index === -1) {\n    // The new layer needs to be reserved in front of all other commutative\n    // keys but after all non-commutative keys (which are added by `forceUpdate`)\n    data.optimisticOrder.unshift(layerKey);\n  } else if (!data.commutativeKeys.has(layerKey)) {\n    // Protect optimistic layers from being turned into non-optimistic layers\n    // while preserving optimistic data\n    clearLayer(data, layerKey);\n    // If the layer was an optimistic layer prior to this call, it'll be converted\n    // to a new non-optimistic layer and shifted ahead\n    data.optimisticOrder.splice(index, 1);\n    data.optimisticOrder.unshift(layerKey);\n  }\n\n  data.commutativeKeys.add(layerKey);\n};\n\n/** Creates an optimistic layer of links and records */\nconst createLayer = (data: InMemoryData, layerKey: number) => {\n  if (data.optimisticOrder.indexOf(layerKey) === -1) {\n    data.optimisticOrder.unshift(layerKey);\n  }\n\n  if (!data.refLock[layerKey]) {\n    data.refLock[layerKey] = makeDict();\n    data.links.optimistic[layerKey] = new Map();\n    data.records.optimistic[layerKey] = new Map();\n  }\n};\n\n/** Clears all links and records of an optimistic layer */\nconst clearLayer = (data: InMemoryData, layerKey: number) => {\n  if (data.refLock[layerKey]) {\n    delete data.refLock[layerKey];\n    delete data.records.optimistic[layerKey];\n    delete data.links.optimistic[layerKey];\n  }\n};\n\n/** Deletes links and records of an optimistic layer, and the layer itself */\nconst deleteLayer = (data: InMemoryData, layerKey: number) => {\n  const index = data.optimisticOrder.indexOf(layerKey);\n  if (index > -1) {\n    data.optimisticOrder.splice(index, 1);\n    data.commutativeKeys.delete(layerKey);\n  }\n\n  clearLayer(data, layerKey);\n};\n\n/** Merges an optimistic layer of links and records into the base data */\nconst squashLayer = (layerKey: number) => {\n  // Hide current dependencies from squashing operations\n  const previousDependencies = currentDependencies;\n  currentDependencies = makeDict();\n\n  const links = currentData!.links.optimistic[layerKey];\n  if (links) {\n    links.forEach((keyMap, entityKey) => {\n      for (const fieldKey in keyMap)\n        writeLink(entityKey, fieldKey, keyMap[fieldKey]);\n    });\n  }\n\n  const records = currentData!.records.optimistic[layerKey];\n  if (records) {\n    records.forEach((keyMap, entityKey) => {\n      for (const fieldKey in keyMap)\n        writeRecord(entityKey, fieldKey, keyMap[fieldKey]);\n    });\n  }\n\n  currentDependencies = previousDependencies;\n  deleteLayer(currentData!, layerKey);\n};\n\n/** Return an array of FieldInfo (info on all the fields and their arguments) for a given entity */\nexport const inspectFields = (entityKey: string): FieldInfo[] => {\n  const { links, records } = currentData!;\n  const fieldInfos: FieldInfo[] = [];\n  const seenFieldKeys: Set<string> = new Set();\n  // Update dependencies\n  updateDependencies(entityKey);\n  // Extract FieldInfos to the fieldInfos array for links and records\n  // This also deduplicates by keeping track of fieldKeys in the seenFieldKeys Set\n  extractNodeMapFields(fieldInfos, seenFieldKeys, entityKey, links);\n  extractNodeMapFields(fieldInfos, seenFieldKeys, entityKey, records);\n  return fieldInfos;\n};\n\nexport const persistData = () => {\n  if (currentData!.storage) {\n    currentIgnoreOptimistic = true;\n    const entries: SerializedEntries = makeDict();\n    currentData!.persist.forEach(key => {\n      const { entityKey, fieldKey } = deserializeKeyInfo(key);\n      let x: void | Link | EntityField;\n      if ((x = readLink(entityKey, fieldKey)) !== undefined) {\n        entries[key] = `:${stringifyVariables(x)}`;\n      } else if ((x = readRecord(entityKey, fieldKey)) !== undefined) {\n        entries[key] = stringifyVariables(x);\n      } else {\n        entries[key] = undefined;\n      }\n    });\n\n    currentIgnoreOptimistic = false;\n    currentData!.storage.writeData(entries);\n    currentData!.persist.clear();\n  }\n};\n\nexport const hydrateData = (\n  data: InMemoryData,\n  storage: StorageAdapter,\n  entries: SerializedEntries\n) => {\n  initDataState('read', data, null);\n\n  for (const key in entries) {\n    const value = entries[key];\n    if (value !== undefined) {\n      const { entityKey, fieldKey } = deserializeKeyInfo(key);\n      if (value[0] === ':') {\n        writeLink(entityKey, fieldKey, JSON.parse(value.slice(1)));\n      } else {\n        writeRecord(entityKey, fieldKey, JSON.parse(value));\n      }\n    }\n  }\n\n  clearDataState();\n  data.storage = storage;\n};\n","import {\n  buildClientSchema,\n  DocumentNode,\n  IntrospectionQuery,\n  GraphQLSchema,\n} from 'graphql';\nimport { createRequest } from '@urql/core';\n\nimport {\n  Cache,\n  FieldInfo,\n  ResolverConfig,\n  DataField,\n  Variables,\n  Data,\n  QueryInput,\n  UpdatesConfig,\n  OptimisticMutationConfig,\n  KeyingConfig,\n  DataFields,\n} from '../types';\nimport { invariant } from '../helpers/help';\n\nimport { read, readFragment } from '../operations/query';\nimport { writeFragment, startWrite } from '../operations/write';\nimport { invalidateEntity } from '../operations/invalidate';\nimport { keyOfField } from './keys';\nimport * as InMemoryData from './data';\nimport * as SchemaPredicates from '../ast/schemaPredicates';\n\ntype RootField = 'query' | 'mutation' | 'subscription';\n\nexport interface StoreOpts {\n  updates?: Partial<UpdatesConfig>;\n  resolvers?: ResolverConfig;\n  optimistic?: OptimisticMutationConfig;\n  keys?: KeyingConfig;\n  schema?: IntrospectionQuery;\n}\n\nexport class Store implements Cache {\n  data: InMemoryData.InMemoryData;\n\n  resolvers: ResolverConfig;\n  updates: UpdatesConfig;\n  optimisticMutations: OptimisticMutationConfig;\n  keys: KeyingConfig;\n  schema?: GraphQLSchema;\n\n  rootFields: { query: string; mutation: string; subscription: string };\n  rootNames: { [name: string]: RootField };\n\n  constructor(opts?: StoreOpts) {\n    if (!opts) opts = {};\n\n    this.resolvers = opts.resolvers || {};\n    this.optimisticMutations = opts.optimistic || {};\n    this.keys = opts.keys || {};\n\n    this.updates = {\n      Mutation: (opts.updates && opts.updates.Mutation) || {},\n      Subscription: (opts.updates && opts.updates.Subscription) || {},\n    } as UpdatesConfig;\n\n    let queryName = 'Query';\n    let mutationName = 'Mutation';\n    let subscriptionName = 'Subscription';\n    if (opts.schema) {\n      const schema = (this.schema = buildClientSchema(opts.schema));\n      const queryType = schema.getQueryType();\n      const mutationType = schema.getMutationType();\n      const subscriptionType = schema.getSubscriptionType();\n      if (queryType) queryName = queryType.name;\n      if (mutationType) mutationName = mutationType.name;\n      if (subscriptionType) subscriptionName = subscriptionType.name;\n\n      if (process.env.NODE_ENV !== 'production') {\n        if (this.keys) {\n          SchemaPredicates.expectValidKeyingConfig(this.schema, this.keys);\n        }\n\n        const hasUpdates =\n          Object.keys(this.updates.Mutation).length > 0 ||\n          Object.keys(this.updates.Subscription).length > 0;\n        if (hasUpdates) {\n          SchemaPredicates.expectValidUpdatesConfig(this.schema, this.updates);\n        }\n\n        if (this.resolvers) {\n          SchemaPredicates.expectValidResolversConfig(\n            this.schema,\n            this.resolvers\n          );\n        }\n\n        if (this.optimisticMutations) {\n          SchemaPredicates.expectValidOptimisticMutationsConfig(\n            this.schema,\n            this.optimisticMutations\n          );\n        }\n      }\n    }\n\n    this.rootFields = {\n      query: queryName,\n      mutation: mutationName,\n      subscription: subscriptionName,\n    };\n\n    this.rootNames = {\n      [queryName]: 'query',\n      [mutationName]: 'mutation',\n      [subscriptionName]: 'subscription',\n    };\n\n    this.data = InMemoryData.make(queryName);\n  }\n\n  keyOfField = keyOfField;\n\n  keyOfEntity(data: Data) {\n    const { __typename: typename, id, _id } = data;\n    if (!typename) {\n      return null;\n    } else if (this.rootNames[typename] !== undefined) {\n      return typename;\n    }\n\n    let key: string | null | void;\n    if (this.keys[typename]) {\n      key = this.keys[typename](data);\n    } else if (id !== undefined && id !== null) {\n      key = `${id}`;\n    } else if (_id !== undefined && _id !== null) {\n      key = `${_id}`;\n    }\n\n    return key ? `${typename}:${key}` : null;\n  }\n\n  resolveFieldByKey(entity: Data | string | null, fieldKey: string): DataField {\n    const entityKey =\n      entity !== null && typeof entity !== 'string'\n        ? this.keyOfEntity(entity)\n        : entity;\n    if (entityKey === null) return null;\n    const fieldValue = InMemoryData.readRecord(entityKey, fieldKey);\n    if (fieldValue !== undefined) return fieldValue;\n    const link = InMemoryData.readLink(entityKey, fieldKey);\n    return link ? link : null;\n  }\n\n  resolve(\n    entity: Data | string | null,\n    field: string,\n    args?: Variables\n  ): DataField {\n    return this.resolveFieldByKey(entity, keyOfField(field, args));\n  }\n\n  invalidate(entity: Data | string, field?: string, args?: Variables) {\n    const entityKey =\n      typeof entity === 'string' ? entity : this.keyOfEntity(entity);\n\n    invariant(\n      entityKey,\n      \"Can't generate a key for invalidate(...).\\n\" +\n        'You have to pass an id or _id field or create a custom `keys` field for `' +\n        typeof entity ===\n        'object'\n        ? (entity as Data).__typename\n        : entity + '`.',\n      19\n    );\n\n    invalidateEntity(entityKey, field, args);\n  }\n\n  inspectFields(entity: Data | string | null): FieldInfo[] {\n    const entityKey =\n      entity !== null && typeof entity !== 'string'\n        ? this.keyOfEntity(entity)\n        : entity;\n\n    return entityKey !== null ? InMemoryData.inspectFields(entityKey) : [];\n  }\n\n  updateQuery(\n    input: QueryInput,\n    updater: (data: Data | null) => DataFields | null\n  ): void {\n    const request = createRequest(input.query, input.variables);\n    const output = updater(this.readQuery(request as QueryInput));\n    if (output !== null) {\n      startWrite(this, request, output as Data);\n    }\n  }\n\n  readQuery(input: QueryInput): Data | null {\n    return read(this, createRequest(input.query, input.variables)).data;\n  }\n\n  readFragment(\n    dataFragment: DocumentNode,\n    entity: string | Data,\n    variables?: Variables\n  ): Data | null {\n    return readFragment(this, dataFragment, entity, variables);\n  }\n\n  writeFragment(\n    dataFragment: DocumentNode,\n    data: Data,\n    variables?: Variables\n  ): void {\n    writeFragment(this, dataFragment, data, variables);\n  }\n}\n","import * as InMemoryData from '../store/data';\nimport { Variables } from '../types';\nimport { keyOfField } from '../store';\n\ninterface PartialFieldInfo {\n  fieldKey: string;\n}\n\nexport const invalidateEntity = (\n  entityKey: string,\n  field?: string,\n  args?: Variables\n) => {\n  const fields: PartialFieldInfo[] = field\n    ? [{ fieldKey: keyOfField(field, args) }]\n    : InMemoryData.inspectFields(entityKey);\n\n  for (let i = 0, l = fields.length; i < l; i++) {\n    const { fieldKey } = fields[i];\n    if (InMemoryData.readLink(entityKey, fieldKey) !== undefined) {\n      InMemoryData.writeLink(entityKey, fieldKey, undefined);\n    } else {\n      InMemoryData.writeRecord(entityKey, fieldKey, undefined);\n    }\n  }\n};\n","import { FieldNode, DocumentNode, FragmentDefinitionNode } from 'graphql';\n\nimport {\n  getSelectionSet,\n  getName,\n  SelectionSet,\n  getFragmentTypeName,\n  getFieldAlias,\n} from '../ast';\n\nimport {\n  getFragments,\n  getMainOperation,\n  normalizeVariables,\n  getFieldArguments,\n} from '../ast';\n\nimport {\n  Variables,\n  Data,\n  DataField,\n  Link,\n  OperationRequest,\n  NullArray,\n  Dependencies,\n} from '../types';\n\nimport {\n  Store,\n  getCurrentOperation,\n  getCurrentDependencies,\n  initDataState,\n  clearDataState,\n  joinKeys,\n  keyOfField,\n} from '../store';\n\nimport * as InMemoryData from '../store/data';\nimport { warn, pushDebugNode, popDebugNode } from '../helpers/help';\n\nimport {\n  Context,\n  makeSelectionIterator,\n  ensureData,\n  makeContext,\n  updateContext,\n} from './shared';\n\nimport {\n  isFieldAvailableOnType,\n  isFieldNullable,\n  isListNullable,\n} from '../ast';\n\nexport interface QueryResult {\n  dependencies: Dependencies;\n  partial: boolean;\n  data: null | Data;\n}\n\nexport const query = (\n  store: Store,\n  request: OperationRequest,\n  data?: Data\n): QueryResult => {\n  initDataState('read', store.data, null);\n  const result = read(store, request, data);\n  clearDataState();\n  return result;\n};\n\nexport const read = (\n  store: Store,\n  request: OperationRequest,\n  input?: Data\n): QueryResult => {\n  const operation = getMainOperation(request.query);\n  const rootKey = store.rootFields[operation.operation];\n  const rootSelect = getSelectionSet(operation);\n\n  const ctx = makeContext(\n    store,\n    normalizeVariables(operation, request.variables),\n    getFragments(request.query),\n    rootKey,\n    rootKey\n  );\n\n  if (process.env.NODE_ENV !== 'production') {\n    pushDebugNode(rootKey, operation);\n  }\n\n  let data: Data | undefined = input || ({} as Data);\n  data =\n    rootKey !== ctx.store.rootFields['query']\n      ? readRoot(ctx, rootKey, rootSelect, data)\n      : readSelection(ctx, rootKey, rootSelect, data);\n\n  if (process.env.NODE_ENV !== 'production') {\n    popDebugNode();\n  }\n\n  return {\n    dependencies: getCurrentDependencies(),\n    partial: data === undefined ? false : ctx.partial,\n    data: data === undefined ? null : data,\n  };\n};\n\nconst readRoot = (\n  ctx: Context,\n  entityKey: string,\n  select: SelectionSet,\n  originalData: Data\n): Data => {\n  if (typeof originalData.__typename !== 'string') {\n    return originalData;\n  }\n\n  const iter = makeSelectionIterator(entityKey, entityKey, select, ctx);\n  const data = {} as Data;\n  data.__typename = originalData.__typename;\n\n  let node: FieldNode | void;\n  while ((node = iter.next()) !== undefined) {\n    const fieldAlias = getFieldAlias(node);\n    const fieldValue = originalData[fieldAlias];\n    if (node.selectionSet !== undefined && fieldValue !== null) {\n      const fieldData = ensureData(fieldValue);\n      data[fieldAlias] = readRootField(ctx, getSelectionSet(node), fieldData);\n    } else {\n      data[fieldAlias] = fieldValue;\n    }\n  }\n\n  return data;\n};\n\nconst readRootField = (\n  ctx: Context,\n  select: SelectionSet,\n  originalData: null | Data | NullArray<Data>\n): Data | NullArray<Data> | null => {\n  if (Array.isArray(originalData)) {\n    const newData = new Array(originalData.length);\n    for (let i = 0, l = originalData.length; i < l; i++)\n      newData[i] = readRootField(ctx, select, originalData[i]);\n    return newData;\n  } else if (originalData === null) {\n    return null;\n  }\n\n  // Write entity to key that falls back to the given parentFieldKey\n  const entityKey = ctx.store.keyOfEntity(originalData);\n  if (entityKey !== null) {\n    // We assume that since this is used for result data this can never be undefined,\n    // since the result data has already been written to the cache\n    const fieldValue = readSelection(ctx, entityKey, select, {} as Data);\n    return fieldValue === undefined ? null : fieldValue;\n  } else {\n    return readRoot(ctx, originalData.__typename, select, originalData);\n  }\n};\n\nexport const readFragment = (\n  store: Store,\n  query: DocumentNode,\n  entity: Partial<Data> | string,\n  variables?: Variables\n): Data | null => {\n  const fragments = getFragments(query);\n  const names = Object.keys(fragments);\n  const fragment = fragments[names[0]] as FragmentDefinitionNode;\n  if (fragment === undefined) {\n    warn(\n      'readFragment(...) was called with an empty fragment.\\n' +\n        'You have to call it with at least one fragment in your GraphQL document.',\n      6\n    );\n\n    return null;\n  }\n\n  const typename = getFragmentTypeName(fragment);\n  if (typeof entity !== 'string' && !entity.__typename) {\n    entity.__typename = typename;\n  }\n\n  const entityKey =\n    typeof entity !== 'string'\n      ? store.keyOfEntity({ __typename: typename, ...entity } as Data)\n      : entity;\n\n  if (!entityKey) {\n    warn(\n      \"Can't generate a key for readFragment(...).\\n\" +\n        'You have to pass an `id` or `_id` field or create a custom `keys` config for `' +\n        typename +\n        '`.',\n      7\n    );\n\n    return null;\n  }\n\n  if (process.env.NODE_ENV !== 'production') {\n    pushDebugNode(typename, fragment);\n  }\n\n  const ctx = makeContext(\n    store,\n    variables || {},\n    fragments,\n    typename,\n    entityKey\n  );\n\n  const result =\n    readSelection(ctx, entityKey, getSelectionSet(fragment), {} as Data) ||\n    null;\n\n  if (process.env.NODE_ENV !== 'production') {\n    popDebugNode();\n  }\n\n  return result;\n};\n\nconst readSelection = (\n  ctx: Context,\n  key: string,\n  select: SelectionSet,\n  data: Data,\n  result?: Data\n): Data | undefined => {\n  const { store } = ctx;\n  const isQuery = key === store.rootFields['query'];\n\n  const entityKey = (result && store.keyOfEntity(result)) || key;\n  if (!isQuery && !!ctx.store.rootNames[entityKey]) {\n    warn(\n      'Invalid root traversal: A selection was being read on `' +\n        entityKey +\n        '` which is an uncached root type.\\n' +\n        'The `' +\n        ctx.store.rootFields.mutation +\n        '` and `' +\n        ctx.store.rootFields.subscription +\n        '` types are special ' +\n        'Operation Root Types and cannot be read back from the cache.',\n      25\n    );\n  }\n\n  const typename = !isQuery\n    ? InMemoryData.readRecord(entityKey, '__typename') ||\n      (result && result.__typename)\n    : key;\n\n  if (typeof typename !== 'string') {\n    return;\n  } else if (result && typename !== result.__typename) {\n    warn(\n      'Invalid resolver data: The resolver at `' +\n        entityKey +\n        '` returned an ' +\n        'invalid typename that could not be reconciled with the cache.',\n      8\n    );\n\n    return;\n  }\n\n  // The following closely mirrors readSelection, but differs only slightly for the\n  // sake of resolving from an existing resolver result\n  data.__typename = typename;\n  const iter = makeSelectionIterator(typename, entityKey, select, ctx);\n\n  let node: FieldNode | void;\n  let hasFields = false;\n  let hasPartials = false;\n  while ((node = iter.next()) !== undefined) {\n    // Derive the needed data from our node.\n    const fieldName = getName(node);\n    const fieldArgs = getFieldArguments(node, ctx.variables);\n    const fieldAlias = getFieldAlias(node);\n    const fieldKey = keyOfField(fieldName, fieldArgs);\n    const key = joinKeys(entityKey, fieldKey);\n    const fieldValue = InMemoryData.readRecord(entityKey, fieldKey);\n    const resultValue = result ? result[fieldName] : undefined;\n    const resolvers = store.resolvers[typename];\n\n    if (process.env.NODE_ENV !== 'production' && store.schema && typename) {\n      isFieldAvailableOnType(store.schema, typename, fieldName);\n    }\n\n    // We temporarily store the data field in here, but undefined\n    // means that the value is missing from the cache\n    let dataFieldValue: void | DataField;\n\n    if (resultValue !== undefined && node.selectionSet === undefined) {\n      // The field is a scalar and can be retrieved directly from the result\n      dataFieldValue = resultValue;\n    } else if (\n      getCurrentOperation() === 'read' &&\n      resolvers &&\n      typeof resolvers[fieldName] === 'function'\n    ) {\n      // We have to update the information in context to reflect the info\n      // that the resolver will receive\n      updateContext(ctx, typename, entityKey, key, fieldName);\n\n      // We have a resolver for this field.\n      // Prepare the actual fieldValue, so that the resolver can use it\n      if (fieldValue !== undefined) {\n        data[fieldAlias] = fieldValue;\n      }\n\n      dataFieldValue = resolvers[fieldName](\n        data,\n        fieldArgs || ({} as Data),\n        store,\n        ctx\n      );\n\n      if (node.selectionSet !== undefined) {\n        // When it has a selection set we are resolving an entity with a\n        // subselection. This can either be a list or an object.\n        dataFieldValue = resolveResolverResult(\n          ctx,\n          typename,\n          fieldName,\n          key,\n          getSelectionSet(node),\n          (data[fieldAlias] || {}) as Data,\n          dataFieldValue\n        );\n      }\n\n      if (\n        store.schema &&\n        dataFieldValue === null &&\n        !isFieldNullable(store.schema, typename, fieldName)\n      ) {\n        // Special case for when null is not a valid value for the\n        // current field\n        return undefined;\n      }\n    } else if (node.selectionSet === undefined) {\n      // The field is a scalar but isn't on the result, so it's retrieved from the cache\n      dataFieldValue = fieldValue;\n    } else if (resultValue !== undefined) {\n      // We start walking the nested resolver result here\n      dataFieldValue = resolveResolverResult(\n        ctx,\n        typename,\n        fieldName,\n        key,\n        getSelectionSet(node),\n        data[fieldAlias] as Data,\n        resultValue\n      );\n    } else {\n      // Otherwise we attempt to get the missing field from the cache\n      const link = InMemoryData.readLink(entityKey, fieldKey);\n\n      if (link !== undefined) {\n        dataFieldValue = resolveLink(\n          ctx,\n          link,\n          typename,\n          fieldName,\n          getSelectionSet(node),\n          data[fieldAlias] as Data\n        );\n      } else if (typeof fieldValue === 'object' && fieldValue !== null) {\n        // The entity on the field was invalid but can still be recovered\n        dataFieldValue = fieldValue;\n      }\n    }\n\n    // Now that dataFieldValue has been retrieved it'll be set on data\n    // If it's uncached (undefined) but nullable we can continue assembling\n    // a partial query result\n    if (\n      dataFieldValue === undefined &&\n      store.schema &&\n      isFieldNullable(store.schema, typename, fieldName)\n    ) {\n      // The field is uncached but we have a schema that says it's nullable\n      // Set the field to null and continue\n      hasPartials = true;\n      data[fieldAlias] = null;\n    } else if (dataFieldValue === undefined) {\n      // The field is uncached and not nullable; return undefined\n      return undefined;\n    } else {\n      // Otherwise continue as usual\n      hasFields = true;\n      data[fieldAlias] = dataFieldValue;\n    }\n  }\n\n  if (hasPartials) ctx.partial = true;\n  return isQuery && hasPartials && !hasFields ? undefined : data;\n};\n\nconst resolveResolverResult = (\n  ctx: Context,\n  typename: string,\n  fieldName: string,\n  key: string,\n  select: SelectionSet,\n  prevData: void | null | Data | Data[],\n  result: void | DataField\n): DataField | void => {\n  if (Array.isArray(result)) {\n    const { store } = ctx;\n    // Check whether values of the list may be null; for resolvers we assume\n    // that they can be, since it's user-provided data\n    const _isListNullable =\n      !store.schema || isListNullable(store.schema, typename, fieldName);\n    const data = new Array(result.length);\n    for (let i = 0, l = result.length; i < l; i++) {\n      // Recursively read resolver result\n      const childResult = resolveResolverResult(\n        ctx,\n        typename,\n        fieldName,\n        joinKeys(key, `${i}`),\n        select,\n        // Get the inner previous data from prevData\n        prevData != null ? prevData[i] : undefined,\n        result[i]\n      );\n\n      if (childResult === undefined && !_isListNullable) {\n        return undefined;\n      } else {\n        data[i] = childResult !== undefined ? childResult : null;\n      }\n    }\n\n    return data;\n  } else if (result === null || result === undefined) {\n    return result;\n  } else if (prevData === null) {\n    // If we've previously set this piece of data to be null,\n    // we skip it and return null immediately\n    return null;\n  } else if (isDataOrKey(result)) {\n    const data = (prevData || {}) as Data;\n    return typeof result === 'string'\n      ? readSelection(ctx, result, select, data)\n      : readSelection(ctx, key, select, data, result);\n  } else {\n    warn(\n      'Invalid resolver value: The field at `' +\n        key +\n        '` is a scalar (number, boolean, etc)' +\n        ', but the GraphQL query expects a selection set for this field.',\n      9\n    );\n\n    return undefined;\n  }\n};\n\nconst resolveLink = (\n  ctx: Context,\n  link: Link | Link[],\n  typename: string,\n  fieldName: string,\n  select: SelectionSet,\n  prevData: void | null | Data | Data[]\n): DataField | undefined => {\n  if (Array.isArray(link)) {\n    const { store } = ctx;\n    const _isListNullable =\n      store.schema && isListNullable(store.schema, typename, fieldName);\n    const newLink = new Array(link.length);\n    for (let i = 0, l = link.length; i < l; i++) {\n      const childLink = resolveLink(\n        ctx,\n        link[i],\n        typename,\n        fieldName,\n        select,\n        prevData != null ? prevData[i] : undefined\n      );\n      if (childLink === undefined && !_isListNullable) {\n        return undefined;\n      } else {\n        newLink[i] = childLink !== undefined ? childLink : null;\n      }\n    }\n\n    return newLink;\n  } else if (link === null || prevData === null) {\n    // If the link is set to null or we previously set this piece of data to be null,\n    // we skip it and return null immediately\n    return null;\n  } else {\n    return readSelection(ctx, link, select, (prevData || {}) as Data);\n  }\n};\n\nconst isDataOrKey = (x: any): x is string | Data =>\n  typeof x === 'string' ||\n  (typeof x === 'object' && typeof (x as any).__typename === 'string');\n","import { IntrospectionQuery } from 'graphql';\n\nimport {\n  Exchange,\n  formatDocument,\n  Operation,\n  OperationResult,\n  RequestPolicy,\n  CacheOutcome,\n} from '@urql/core';\n\nimport {\n  filter,\n  map,\n  merge,\n  pipe,\n  share,\n  fromPromise,\n  fromArray,\n  buffer,\n  take,\n  mergeMap,\n  concat,\n  empty,\n  Source,\n} from 'wonka';\n\nimport { query, write, writeOptimistic } from './operations';\nimport { makeDict, isDictEmpty } from './helpers/dict';\nimport { filterVariables, getMainOperation } from './ast';\nimport { Store, noopDataState, hydrateData, reserveLayer } from './store';\n\nimport {\n  UpdatesConfig,\n  ResolverConfig,\n  OptimisticMutationConfig,\n  KeyingConfig,\n  StorageAdapter,\n  Dependencies,\n} from './types';\n\ntype OperationResultWithMeta = OperationResult & {\n  outcome: CacheOutcome;\n  dependencies: Dependencies;\n};\n\ntype Operations = Set<number>;\ntype OperationMap = Map<number, Operation>;\ntype OptimisticDependencies = Map<number, Dependencies>;\ntype DependentOperations = Record<string, number[]>;\n\n// Returns the given operation result with added cacheOutcome meta field\nconst addCacheOutcome = (op: Operation, outcome: CacheOutcome): Operation => ({\n  ...op,\n  context: {\n    ...op.context,\n    meta: {\n      ...op.context.meta,\n      cacheOutcome: outcome,\n    },\n  },\n});\n\n// Copy an operation and change the requestPolicy to skip the cache\nconst toRequestPolicy = (\n  operation: Operation,\n  requestPolicy: RequestPolicy\n): Operation => ({\n  ...operation,\n  context: {\n    ...operation.context,\n    requestPolicy,\n  },\n});\n\nexport interface CacheExchangeOpts {\n  updates?: Partial<UpdatesConfig>;\n  resolvers?: ResolverConfig;\n  optimistic?: OptimisticMutationConfig;\n  keys?: KeyingConfig;\n  schema?: IntrospectionQuery;\n  storage?: StorageAdapter;\n}\n\nexport const cacheExchange = (opts?: CacheExchangeOpts): Exchange => ({\n  forward,\n  client,\n  dispatchDebug,\n}) => {\n  const store = new Store(opts);\n\n  let hydration: void | Promise<void>;\n  if (opts && opts.storage) {\n    hydration = opts.storage.readData().then(entries => {\n      hydrateData(store.data, opts!.storage!, entries);\n    });\n  }\n\n  const optimisticKeysToDependencies: OptimisticDependencies = new Map();\n  const mutationResultBuffer: OperationResult[] = [];\n  const ops: OperationMap = new Map();\n  const blockedDependencies: Dependencies = makeDict();\n  const requestedRefetch: Operations = new Set();\n  const deps: DependentOperations = makeDict();\n\n  const isBlockedByOptimisticUpdate = (dependencies: Dependencies): boolean => {\n    for (const dep in dependencies) if (blockedDependencies[dep]) return true;\n    return false;\n  };\n\n  const collectPendingOperations = (\n    pendingOperations: Operations,\n    dependencies: void | Dependencies\n  ) => {\n    if (dependencies) {\n      // Collect operations that will be updated due to cache changes\n      for (const dep in dependencies) {\n        const keys = deps[dep];\n        if (keys) {\n          deps[dep] = [];\n          for (let i = 0, l = keys.length; i < l; i++) {\n            pendingOperations.add(keys[i]);\n          }\n        }\n      }\n    }\n  };\n\n  const executePendingOperations = (\n    operation: Operation,\n    pendingOperations: Operations\n  ) => {\n    // Reexecute collected operations and delete them from the mapping\n    pendingOperations.forEach(key => {\n      if (key !== operation.key) {\n        const op = ops.get(key);\n        if (op) {\n          ops.delete(key);\n          let policy: RequestPolicy = 'cache-first';\n          if (requestedRefetch.has(key)) {\n            requestedRefetch.delete(key);\n            policy = 'cache-and-network';\n          }\n          client.reexecuteOperation(toRequestPolicy(op, policy));\n        }\n      }\n    });\n  };\n\n  // This registers queries with the data layer to ensure commutativity\n  const prepareForwardedOperation = (operation: Operation) => {\n    if (operation.operationName === 'query') {\n      // Pre-reserve the position of the result layer\n      reserveLayer(store.data, operation.key);\n    } else if (operation.operationName === 'teardown') {\n      // Delete reference to operation if any exists to release it\n      ops.delete(operation.key);\n      // Mark operation layer as done\n      noopDataState(store.data, operation.key);\n    } else if (\n      operation.operationName === 'mutation' &&\n      operation.context.requestPolicy !== 'network-only'\n    ) {\n      // This executes an optimistic update for mutations and registers it if necessary\n      const { dependencies } = writeOptimistic(store, operation, operation.key);\n      if (!isDictEmpty(dependencies)) {\n        // Update blocked optimistic dependencies\n        for (const dep in dependencies) {\n          blockedDependencies[dep] = true;\n        }\n\n        // Store optimistic dependencies for update\n        optimisticKeysToDependencies.set(operation.key, dependencies);\n\n        // Update related queries\n        const pendingOperations: Operations = new Set();\n        collectPendingOperations(pendingOperations, dependencies);\n        executePendingOperations(operation, pendingOperations);\n      }\n    }\n\n    return {\n      ...operation,\n      variables: operation.variables\n        ? filterVariables(\n            getMainOperation(operation.query),\n            operation.variables\n          )\n        : operation.variables,\n      query: formatDocument(operation.query),\n    };\n  };\n\n  // This updates the known dependencies for the passed operation\n  const updateDependencies = (op: Operation, dependencies: Dependencies) => {\n    for (const dep in dependencies) {\n      (deps[dep] || (deps[dep] = [])).push(op.key);\n      ops.set(op.key, op);\n    }\n  };\n\n  // Retrieves a query result from cache and adds an `isComplete` hint\n  // This hint indicates whether the result is \"complete\" or not\n  const operationResultFromCache = (\n    operation: Operation\n  ): OperationResultWithMeta => {\n    const res = query(store, operation);\n    const cacheOutcome: CacheOutcome = res.data\n      ? !res.partial\n        ? 'hit'\n        : 'partial'\n      : 'miss';\n\n    updateDependencies(operation, res.dependencies);\n\n    return {\n      outcome: cacheOutcome,\n      operation,\n      data: res.data,\n      dependencies: res.dependencies,\n    };\n  };\n\n  // Take any OperationResult and update the cache with it\n  const updateCacheWithResult = (\n    result: OperationResult,\n    pendingOperations: Operations\n  ): OperationResult => {\n    const { operation, error, extensions } = result;\n    const { key } = operation;\n\n    if (operation.operationName === 'mutation') {\n      // Collect previous dependencies that have been written for optimistic updates\n      const dependencies = optimisticKeysToDependencies.get(key);\n      collectPendingOperations(pendingOperations, dependencies);\n      optimisticKeysToDependencies.delete(key);\n    } else {\n      reserveLayer(store.data, operation.key);\n    }\n\n    let queryDependencies: void | Dependencies;\n    if (result.data) {\n      // Write the result to cache and collect all dependencies that need to be\n      // updated\n      const writeDependencies = write(store, operation, result.data, key)\n        .dependencies;\n      collectPendingOperations(pendingOperations, writeDependencies);\n\n      const queryResult = query(store, operation, result.data);\n      result.data = queryResult.data;\n      if (operation.operationName === 'query') {\n        // Collect the query's dependencies for future pending operation updates\n        queryDependencies = queryResult.dependencies;\n        collectPendingOperations(pendingOperations, queryDependencies);\n      }\n    } else {\n      noopDataState(store.data, operation.key);\n    }\n\n    // Update this operation's dependencies if it's a query\n    if (queryDependencies) {\n      updateDependencies(result.operation, queryDependencies);\n    }\n\n    return { data: result.data, error, extensions, operation };\n  };\n\n  return ops$ => {\n    const sharedOps$ = pipe(ops$, share);\n\n    // Buffer operations while waiting on hydration to finish\n    // If no hydration takes place we replace this stream with an empty one\n    const bufferedOps$ = hydration\n      ? pipe(\n          sharedOps$,\n          buffer(fromPromise(hydration)),\n          take(1),\n          mergeMap(fromArray)\n        )\n      : (empty as Source<Operation>);\n\n    const inputOps$ = pipe(concat([bufferedOps$, sharedOps$]), share);\n\n    // Filter by operations that are cacheable and attempt to query them from the cache\n    const cacheOps$ = pipe(\n      inputOps$,\n      filter(op => {\n        return (\n          op.operationName === 'query' &&\n          op.context.requestPolicy !== 'network-only'\n        );\n      }),\n      map(operationResultFromCache),\n      share\n    );\n\n    const nonCacheOps$ = pipe(\n      inputOps$,\n      filter(op => {\n        return (\n          op.operationName !== 'query' ||\n          op.context.requestPolicy === 'network-only'\n        );\n      })\n    );\n\n    // Rebound operations that are incomplete, i.e. couldn't be queried just from the cache\n    const cacheMissOps$ = pipe(\n      cacheOps$,\n      filter(res => {\n        return (\n          res.outcome === 'miss' &&\n          res.operation.context.requestPolicy !== 'cache-only' &&\n          !isBlockedByOptimisticUpdate(res.dependencies)\n        );\n      }),\n      map(res => {\n        dispatchDebug({\n          type: 'cacheMiss',\n          message: 'The result could not be retrieved from the cache',\n          operation: res.operation,\n        });\n        return addCacheOutcome(res.operation, 'miss');\n      })\n    );\n\n    // Resolve OperationResults that the cache was able to assemble completely and trigger\n    // a network request if the current operation's policy is cache-and-network\n    const cacheResult$ = pipe(\n      cacheOps$,\n      filter(\n        res =>\n          res.outcome !== 'miss' ||\n          res.operation.context.requestPolicy === 'cache-only'\n      ),\n      map(\n        (res: OperationResultWithMeta): OperationResult => {\n          const { operation, outcome, dependencies } = res;\n          const result: OperationResult = {\n            operation: addCacheOutcome(operation, outcome),\n            data: res.data,\n            error: res.error,\n            extensions: res.extensions,\n          };\n\n          if (\n            operation.context.requestPolicy === 'cache-and-network' ||\n            (operation.context.requestPolicy === 'cache-first' &&\n              outcome === 'partial')\n          ) {\n            result.stale = true;\n            if (!isBlockedByOptimisticUpdate(dependencies)) {\n              client.reexecuteOperation(\n                toRequestPolicy(operation, 'network-only')\n              );\n            } else if (\n              operation.context.requestPolicy === 'cache-and-network'\n            ) {\n              requestedRefetch.add(operation.key);\n            }\n          }\n\n          dispatchDebug({\n            type: 'cacheHit',\n            message: `A requested operation was found and returned from the cache.`,\n            operation: res.operation,\n            data: {\n              value: result,\n            },\n          });\n\n          return result;\n        }\n      )\n    );\n\n    // Forward operations that aren't cacheable and rebound operations\n    // Also update the cache with any network results\n    const result$ = pipe(\n      merge([nonCacheOps$, cacheMissOps$]),\n      map(prepareForwardedOperation),\n      forward,\n      share\n    );\n\n    // Results that can immediately be resolved\n    const nonOptimisticResults$ = pipe(\n      result$,\n      filter(result => !optimisticKeysToDependencies.has(result.operation.key)),\n      map(result => {\n        const pendingOperations: Operations = new Set();\n        // Update the cache with the incoming API result\n        const cacheResult = updateCacheWithResult(result, pendingOperations);\n        // Execute all dependent queries\n        executePendingOperations(result.operation, pendingOperations);\n        return cacheResult;\n      })\n    );\n\n    // Prevent mutations that were previously optimistic from being flushed\n    // immediately and instead clear them out slowly\n    const optimisticMutationCompletion$ = pipe(\n      result$,\n      filter(result => optimisticKeysToDependencies.has(result.operation.key)),\n      mergeMap(\n        (result: OperationResult): Source<OperationResult> => {\n          const length = mutationResultBuffer.push(result);\n          if (length < optimisticKeysToDependencies.size) {\n            return empty;\n          }\n\n          for (let i = 0; i < mutationResultBuffer.length; i++) {\n            reserveLayer(store.data, mutationResultBuffer[i].operation.key);\n          }\n\n          for (const dep in blockedDependencies) {\n            delete blockedDependencies[dep];\n          }\n\n          const results: OperationResult[] = [];\n          const pendingOperations: Operations = new Set();\n\n          let bufferedResult: OperationResult | void;\n          while ((bufferedResult = mutationResultBuffer.shift()))\n            results.push(\n              updateCacheWithResult(bufferedResult, pendingOperations)\n            );\n\n          // Execute all dependent queries as a single batch\n          executePendingOperations(result.operation, pendingOperations);\n\n          return fromArray(results);\n        }\n      )\n    );\n\n    return merge([\n      nonOptimisticResults$,\n      optimisticMutationCompletion$,\n      cacheResult$,\n    ]);\n  };\n};\n","import { stringifyVariables } from '@urql/core';\nimport { Variables, FieldInfo, KeyInfo } from '../types';\n\nexport const keyOfField = (fieldName: string, args?: null | Variables) =>\n  args ? `${fieldName}(${stringifyVariables(args)})` : fieldName;\n\nexport const joinKeys = (parentKey: string, key: string) =>\n  `${parentKey}.${key}`;\n\nexport const fieldInfoOfKey = (fieldKey: string): FieldInfo => {\n  const parenIndex = fieldKey.indexOf('(');\n  if (parenIndex > -1) {\n    return {\n      fieldKey,\n      fieldName: fieldKey.slice(0, parenIndex),\n      arguments: JSON.parse(fieldKey.slice(parenIndex + 1, -1)),\n    };\n  } else {\n    return {\n      fieldKey,\n      fieldName: fieldKey,\n      arguments: null,\n    };\n  }\n};\n\nexport const serializeKeys = (entityKey: string, fieldKey: string) =>\n  `${entityKey.replace(/\\./g, '%2e')}.${fieldKey}`;\n\nexport const deserializeKeyInfo = (key: string): KeyInfo => {\n  const dotIndex = key.indexOf('.');\n  const entityKey = key.slice(0, dotIndex).replace(/%2e/g, '.');\n  const fieldKey = key.slice(dotIndex + 1);\n  return { entityKey, fieldKey };\n};\n","export const makeDict = (): any => Object.create(null);\n\nexport const isDictEmpty = (x: any) => {\n  for (const _ in x) return false;\n  return true;\n};\n","import { pipe, filter } from 'wonka';\nimport { print, SelectionNode } from 'graphql';\n\nimport {\n  Operation,\n  GraphQLRequest,\n  Exchange,\n  CombinedError,\n  createRequest,\n} from '@urql/core';\n\nimport {\n  getMainOperation,\n  getFragments,\n  isInlineFragment,\n  isFieldNode,\n  shouldInclude,\n  getSelectionSet,\n  getName,\n} from './ast';\n\nimport { makeDict } from './helpers/dict';\nimport { OptimisticMutationConfig, Variables } from './types';\nimport { cacheExchange, CacheExchangeOpts } from './cacheExchange';\n\n/** Determines whether a given query contains an optimistic mutation field */\nconst isOptimisticMutation = (\n  config: OptimisticMutationConfig,\n  operation: Operation\n) => {\n  const vars: Variables = operation.variables || makeDict();\n  const fragments = getFragments(operation.query);\n  const selections = [...getSelectionSet(getMainOperation(operation.query))];\n\n  let field: void | SelectionNode;\n  while ((field = selections.pop())) {\n    if (!shouldInclude(field, vars)) {\n      continue;\n    } else if (!isFieldNode(field)) {\n      const fragmentNode = !isInlineFragment(field)\n        ? fragments[getName(field)]\n        : field;\n      if (fragmentNode) selections.push(...getSelectionSet(fragmentNode));\n    } else if (config[getName(field)]) {\n      return true;\n    }\n  }\n\n  return false;\n};\n\nconst isOfflineError = (error: undefined | CombinedError) =>\n  error &&\n  error.networkError &&\n  !error.response &&\n  ((typeof navigator !== 'undefined' && navigator.onLine === false) ||\n    /request failed|failed to fetch|network\\s?error/i.test(\n      error.networkError.message\n    ));\n\nexport const offlineExchange = (opts: CacheExchangeOpts): Exchange => ({\n  forward: outerForward,\n  client,\n  dispatchDebug,\n}) => {\n  const { storage } = opts;\n  let forward = outerForward;\n\n  if (\n    storage &&\n    storage.onOnline &&\n    storage.readMetadata &&\n    storage.writeMetadata\n  ) {\n    const optimisticMutations = opts.optimistic || {};\n    const failedQueue: GraphQLRequest[] = [];\n\n    const updateMetadata = () => {\n      storage.writeMetadata!(\n        failedQueue.map(op => ({\n          query: print(op.query),\n          variables: op.variables,\n        }))\n      );\n    };\n\n    let _flushing = false;\n    const flushQueue = () => {\n      let request: void | GraphQLRequest;\n      while (!_flushing && (request = failedQueue.shift())) {\n        _flushing = true;\n        client.dispatchOperation(\n          client.createRequestOperation('mutation', request)\n        );\n        _flushing = false;\n      }\n\n      updateMetadata();\n    };\n\n    forward = ops$ => {\n      return pipe(\n        outerForward(ops$),\n        filter(res => {\n          if (\n            res.operation.operationName === 'subscription' ||\n            !isOfflineError(res.error)\n          ) {\n            return true;\n          } else if (res.operation.operationName === 'mutation') {\n            if (isOptimisticMutation(optimisticMutations, res.operation)) {\n              failedQueue.push(res.operation);\n              updateMetadata();\n              return false;\n            } else {\n              return true;\n            }\n          } else {\n            return false;\n          }\n        })\n      );\n    };\n\n    storage.onOnline(flushQueue);\n    storage.readMetadata().then(mutations => {\n      if (mutations)\n        for (let i = 0; i < mutations.length; i++)\n          failedQueue.push(\n            createRequest(mutations[i].query, mutations[i].variables)\n          );\n      flushQueue();\n    });\n  }\n\n  return cacheExchange(opts)({\n    forward,\n    client,\n    dispatchDebug,\n  });\n};\n"],"names":["a","getField","ofType","isListType","isFieldNode","indexStack","select","records","resolver","node","Kind","argsSize","vars","def","c","Set","doc","getName","map","getFragments","fieldName","expectObjectType","schema","object","x","Object","updates","queryType","validQueries","validTypeProperties","refCount","link","entityKey","extractNodeFields","currentData","gc","fieldKey","updateDependencies","data","layerKey","clearLayer","squashLayer","currentDependencies","previousDependencies","deleteLayer","extractNodeMapFields","initDataState","deserializeKeyInfo","ctx","typeCondition","store","b","result","startWrite","makeContext","getSelectionSet","writeSelection","process","fragments","getFragmentTypeName","makeSelectionIterator","iter","getFieldAlias","undefined","parentFieldKey","writeField","newData","childKey","forEach","fields","i","opts","subscriptionType","read","query","dependencies","originalData","entity","names","typename","readSelection","warn","dataFieldValue","hasPartials","hasFields","Array","isListNullable","m","newLink","childLink","_extends","res","getSubscriptionType","results","updateCacheWithResult","const","parenIndex","f","currentIgnoreOptimistic","currentOperation","persistData","k","optimisticKeysToDependencies","this","keys","pendingOperations","reserveLayer","Store","operation","writeOptimistic","deps","dep","createRequest","ops","op","noopDataState","fromArray","share","concat","filter","merge","outerForward","failedQueue","mutations","updateMetadata","storage"],"mappings":";;;;UAoESA;;;;;;mBCzCOC;;;AAWRC;SACCC,sBAAWD;;;;;;;;;;;;;;;;SC+CXE,WAAAA;SAwDOC;;;;;MA5CRA,IAAuB;;;eAQVC;;;;gBA0CkBC;;;WANvBF,KAAAA;;;;;;;ACsGJG,SAAAA;;;;;;AADsB;OAW1BC;;;;;;;;;;;;;;;;;;;;;;;;oTC9O4BA,qJAKrBA;;;SAkBkBA,gBAAcC,KAAAA;;qBCtBvBD;;sDAMhBE;;UAKCA,IAAAA;;;;;;0BA0BDC,IAAO;;;+FAKGH,gBACSI;;;;;;MCxBXC;;SAAAA,yCACEC;;;;;;eNrBhBN;;;;;;QAAAA;;;;wBAMkBO;;;GAiBZC,UAAJC,sEAFSC;SACXH;;MAUIF;;;;;oBCZqCM;;;mBAoBrCpB;;;uBAWJqB;;eASeC;OAGDC;;;;wCAsBZC,0CAcqCA;;;;;sBAcvBC;;;;;6BAiCZA,MAAYC;;;;;;;;;;;;YA8CNC;;;oBAIEC;;;;;;;;WAWAC,eACHP;;;;;;;;uBMiEeQ,GAAUC;;;YAGxBC;;;;;;;;;;;mBAkCVC,OAA6Cf;wBAITgB;oBAEhCD;;;;;;gBAqBcH,OAAuB;;gBAI9BA;;;;sBASPI;;IA5BOC;;;;;;wBAyDmBC;;;;;;2CA4C5BN;;;;;+BAY4CM;oBAG9CC;;;wBAO8BN;2BAIHO;qCACbA;;;;;;;;;;;YAiCGC,4CAERD,YAAwBC,WACxBD;;;YAYTE;;UAYeJ;;;;;;;;;IARXK;;qBAGJC;;;sBAkBsBC;sCACtBC;;;sBAaAC,GAAAA,GAAAA;;;;;;;;;uBAqBEX,YAAAA;;;qBAUFY,SAAAA;EAAAA;;;mBAKoCC;;WACtBjC;;;;;kBLvhBdkC;;;;;;;;oBAciBC;;;;wBCZMC,YAAmBC;yBACpCC;;GAKKC;gBASCC;;iBAYGN,qBAAoBO,GAAnCC;;;oCAcwCxD,GAAxC8C;iBAG4B;;mBAaxBW;;;;;eA6BctC;MAEDuC;;;;UASAC;;;;;;mBAiBLL;;;;;UA0BLtD;+BAIM4D,QAAAA,KAIXZ,EAJIa;;oBAQSA;;2BAIMC;kCAGfL;gBAsBST;;iBAfee;;;;;;wBAyFTC;OAIHC,YAAAA,MAAAA,kBAEdC,EAAAA;;;;;6BAQclB,yBACDV;;;uBA2BahC,GAAQgC;SAC/B6B;;;;;wBK1SoBC;4CCvCJC,EAAOC;sBDkClBC,gBCjCyCR;;;;;;;;;;0BDuDRS;;;;;;;;;;;;;;;oBAqEJ;eAGnCxC;;;;;;;;;;;;;;;;;;;SAsDGyC;;;;;;GE5IEC;iBAMID,IAAAA,EAAKvB;;;kBAMpBA;yCASEA;;YAaYF,OAAAA,sCAC2BV;eAQvCqC;;;;;;;cAkBgBC,WAAlBtC;;;;;;yBAyB+BU;;;gBAExB;;;;yBAiBPE,GACAwB,GACAG;MAGkB1D,+BACZ2D,IAAQrD;UACmBX;;;MAW3BiE;2BAC6BF;2CAM7B3B;;;;iCAuBJ6B,IACA/C;MAIAgD,iBAAmBhD;;;;;yCAoCJ;mBAKbyB;;;;;;;;;mDA4CKwB;;;;gBA6C0BlB;;;;;;;iBAmDZmB;;;SAqCCnB,GAGbO,qBAnCOa,MAAgBC;MAY9BC;;QAKAzB,oCAAiB0B;;MAeGvB,IAAAA,YAGbO;;;;;UAKJtE;;;UAEAuF;;;;;;;;;;YA+CDC,OAA2BzB,WAAY0B;;;;;;;;;SCzb+BC;;8BAAAA;;;;;;;eAuRtEC;;;;;mBA7OUzC,QAAYqB;;;;;;yBAyNpBoB;;;;;;;;;QAmFuBvC;;;;;;;;;;;;UAyBYwC;UACG9E,IAAIC;;cAIxC8E,EACEC;2DCtaPC;UAOCC,iBAAa5D;;2CANThB;;;;uIA4BJgB;MAAAA,aC5BC6D;;;;;;;UAAAA;oDNsDLvD,sBAA2CuC,mCAE3CiB;;0BAcFC;oBACc7D;;;oDAyEV8D;MAAAA;;;;;;2CAjBA3D;;yBAMJC,cAAAA;;;gCAoBAJ;;;;;;;wEAiC2D+D;;;;;iHAsDtCnF;;;;UAcdA;;;;;;;cIiJcoF;;;;;;;;;;;;;iBA7RLC,eAEYC;;;;uBAU1BC;;;;;;;;;;;AAuBEC;;;;AACKC;6BAMLC;wCAIyBC;;;;;gCAQvBP,KAAiCM,kDAGS7F;;;AAVZ4F;;;;;;SA+B/BG,WAAmBC,KAAOC,gBAC3BC,OAAAA,EAAQC;;;+BAUyBvB,SAC9BA;SAKLtD;;;;yBAkBIuE;;;;;;;;;;qBAyBFO;;WAKA9E;cAGOC,MAAMc,4BAARpD;;;;iCAaQoH;eAI4CC,MAAAA;;uBAmCzDnG;MG9RoB0F,GAELU;2BH0SfC;;;kCA0GKC;;;UGpZUF;;;;;;;;;;;;;;;;;;;;;;;mCAsEbG,EAAAA;;;WA0BEC,yBACgBC,IAAAA,YAAoBA;;;;;;;;4BAvDK3H,YAGzC4H;iEA+CNC,EAAAA,yCACAA;;iDAUmBtD,GAAM;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;"}